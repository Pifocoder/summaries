## leaderless репликация
У нас n узлов.
Писать синхронно мы будем в $\lfloor{n/2}\rfloor$ + 1 
Писать асинхронно мы будем в остальные ноды
Читаем из  $\lfloor{n/2}\rfloor$ + 1 
quarum =  $\lfloor{n/2}\rfloor$ + 1 
В такой схеме мы можем пережить $(n - (\lfloor{n/2}\rfloor + 1))$ отказов.

Для каждого клиента мы храним counter и id, counter клиента увеличивается при каждом начале запросе.
А при каждой записи мы получаем Get с версией и потом делаем Set с новым значением, с версией + 1 и с id и counter клиента, который записывает. Это позволяет хорошо сортировать параллельные запросы и одного клиента, и нескольких.

Есть проблема, два последовательных get, которые вложены в set, который ставит 1, могут увидеть разные значения:
1) первый увидит 1, потому что будет чистать с обновленных машин.
2) второй увидит 0, потому что будет читать не с обновленных машин.
Фиксится тем, что во время get будет выполняться set, тем самым get растянется. и последовательных get-ов не будет, так как когда первый get закончится, значение будет обновлено. Когда мы делаем set в get, версия не обновляется.

## Single/multi leader
CRDT - структура даннх, которая позволяет писать не из одного места, будет определенная корректность.
#### Задача консенсуса
Свойства решения такой задачи:
1) Валидность, должно быть выбрано значение, которое кто то предлагал
2) Согласованность - все согласны на выбранное значение
3) Конечность процедуры
Paxos - алгоритм для решения этой задачи.

Алгоритм Paxos для выборо одного значения:
У узлов есть роли: proposer, acceptor
n - ballot number
Acceptor хранит:
1) n_p - последний номер бюллетеня, который он видел
2) n_a, v_a - бюллетень и value, за которое Acceptor проголосовал в последний раз
Алгоритм:
1) Proposer отправляет prepare(n)
2) Acceptor отколняет пришедшее n, если оно меньш или равно n_p
3) Acceptor принимает пришедшее n, отправляет результат Proposer-у, то есть n_a, v_a 
4) Proposer выбирает v_a из пришедших с максимальным n_a
5) Proposer отправляет выбранное value, т е запрос Accept(n, value) на голосование Acceptor-ам
6) Acceptor отправляет Proposer-у подтверждение и обновляет свои n_a и v_a
Теорема
если пара (n, v) лежит на кворуме, то это значение будет выбрано.

FLP теорема
В плохой сети за конечное время может не решаться задача консенсуса.

Чтобы решить эту проблему можно экспоненциально увеличивать время задержки после файла, а также добавлять eps.

### Multi paxos
Replicate State Machine - RSM
Каждый узел является и acceptor и proposer и разносит лог по всем, каждая ячейка лога это отдельное голосование.
Возможные состояния ячейки лога acceptor, то есть состояние i-го голосания:
1) accepted (n_a, v_a), но очевидно, что может быть ещё не chosen
2) сhosen - то есть значение в ячейке разнеслась по кворуму, в том числе в выбранный узел
3) empty, команда этой ячейки разнеслась по кворуму, но не попала в выбранный узел, если существует chosen команда, то мы её запишем, когда с этого узла отправим propose(nop), он нам вернет реальную команду
На каждом логе двигается commited, перед тем как подвинуть commited, мы делаем propose и двигаем, если команда на chosen в этой ячейке.

**Eventual consistency.**
## Single-leader multi-paxos
Easy to realise: we should choose one machine that will be the leader. The
leader has to send **heartebeats** periodically. If the leader is dead we have
to choose another leader.
'Split brain' is not a problem thanks to the paxos algo.
We can choose the machine with the max id as leader. If it receives the
heartbear from the node with the highest id, it stops to be the leader.
## RTT
Round trip: one round trip (forward and backward) across the network.
**4 RTT** if we don't know the leader (to learn the leader, to the leader, propose
(prepare, accept)).
Then **3 RTT**.
## Optimizations
We can split the structure into shards.
We can merge `accept` for slot k and `merge` for slot k + 1 into one message.
We can also send `prepares` to all log cells. If no node has sent us a prepare
with the higher ballot number, the cell is considered as _warmed up_, so we can
send accept immediately.

In total: **2 RTT**.

## Отказы
хотим переживать f отказов, когда f < n / 2. без ограничения общности n = 2 * f + 1
Для этого нам необходимо 2 * f + 1 acceptor, а также f + 1 proposer. Заведем replics, которые будут реально поддерживать состояние, их должно быть f + 1.
Понятное дело, что replics, proposers и acceptors пересекаются по узлам очень сильно.
Также существуют машины batches, которые собирают сообщения клиентов в батчи.

Оптимизации:
1) Сделаем размер кворума на prepare больще, а размер кворума на accept меньше, минимальный размер = f + 1
### Rack awareness
....

## RAFT
В Multi paxos очень много оптимизаций,  поэтому придумали - https://web.stanford.edu/~ouster/cgi-bin/papers/raft-extended.pdf


![](https://i.imgur.com/BETIrZE.png)
каждый цвет новый term
1) сначала был выбран n2 как лидер
2) потом был выбран n1, потому что видно, что у него больше команд
3) Потом был выбран n2 лидером
При несовпадении у нас n2 будет отправлять экспоненциально увеличивающиеся enteries и как только произошло совпаденияе prev_idx, prev_term мы фиксим лог, того кому отправили (перезатираем не совпадающие) и применяем новые операции
![](https://i.imgur.com/Ue3Xl4J.png)

До этого мы смотрели просто на терм и принимали кандидата, если у него больше терм, теперь будет умнее условие, чтобы перезатирать более бесполезные данные:
![J3sp0p6.png|710](https://i.imgur.com/J3sp0p6.png)
Пример работы, когда придет 5 term, например от n3, она не примется, потому что n1, n2 уже видели 4 term. Поэтому примется 5 от n1
![](https://i.imgur.com/NMo5gjT.png)

Когда мы комитим?
![](https://i.imgur.com/ayVcfFz.png)
commit index разносится с помощью постого apply или heart beat

Когда мы отвечаем клиенту, что команда применена?
Когда commit_index разнсен на кворум.

Чтобы добавить узел, будем раз в какое то время делать снэпшот, а потом добавлять узел с этим снэпшотом и потом добавлять команды.