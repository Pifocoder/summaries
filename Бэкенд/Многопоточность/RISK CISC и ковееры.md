Стадии:
1) fetch
2) decode
3) execution
4) memory access
5) write back

## Проблемы обычного конвеера(Hazards)
### Data hazards:
1) read after write
2) write after read
3) write after write
example:
```
add r1, r2, r3
add r3, r2, r4
add r4, r2, r5
```

## Branch prediction:

#### if в ассемблере
`jz r1, r2`

если r4 == r5 будет прыжок в r2.
```
cmp r4, r5
je r2
```

Ветвления очень мешают исполняться, пример:
```
cmp r1, r3
	je r4
add r2, r5, r6
```
тут сначала нужно дождаться add перед jump

## Out of order
Идея: попробуем исполнять инструкции в произволном порядке

load/store buffer -  очередь, в которую будем склыдывать зависимости на данные.
reorder buffer - очередь, которая приводит все к изначальному порядку (который был в программе), то есть если мы испольнили mul раньше add(потому что у них  нет общих данных), а в программе add шел раньше, то нам надо откатить порядок к начальному.

Тэги:
```
mul r2 r4 r3
add r3 r4 r5
```
Так как add-у нужен r3, r3 помечается тэгом t1
В шину после исполнения mul кидается тэг t1 и значение mul и исполняется add.
Идейно похоже на future.

Связь load и store buffer:
с помощью очереди, которая объединяет load и store buffer:
```
store ...
load  ...
store ...
```

Пример:
```
1) mul r1 r2 r3
2) add r2 r4 r5
3) st r7 8(r3)
4) st r6 0(r2)
```
reorder buffer итерации:
1) 2
2) 2 4
3) 1 2 4 
выкидываем из reorder buffer 1 и 2
4) 4
5) 3 4
выкидываем из reorder buffer 3 и 4

### fetch и decode оптимизации
#### Fetch
лучше делать fetch например по 16 байт(так в X86), чтобы было не много(иначе все осядет в буфферах) и не мало(иначе будем брять постоянно неплные инструкции)

#### Decode:
predecode:
1) если пришла большая инструкция, то её надо сохранить в очереди и ждать, когда придет остальная часть
2) macro fusion - слияние инструкций:
```
cmp
je
move
```
перейдет в:
```
cmp 
cmovne
```
3) micro fusion - нечто аналогично, но с памятью.

>[!info]
> branch predictor работает на уровне decode и fetch.

